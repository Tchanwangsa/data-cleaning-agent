{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ec261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necesary packages\n",
    "%pip install nbimporter\n",
    "%pip install nbformat\n",
    "\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install langchain langchain-community openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd337c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbimporter import NotebookLoader\n",
    "import sys\n",
    "sys.meta_path.append(NotebookLoader())\n",
    "\n",
    "# Lang chain imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI  \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Import features (TODO: Add features as we develop)\n",
    "from summaries import get_summaries\n",
    "from outliers import detect_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8051915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(user_query, df):\n",
    "    log = [] # To be implemented later\n",
    "\n",
    "    # LLM Prompt\n",
    "    prompt = f\"\"\"\n",
    "    User wants: \"{user_query}\"\n",
    "    Dataset: {df.shape[0]} rows, {df.shape[1]} columns\n",
    "    Which feature(s) should handle this request?\n",
    "\n",
    "    Available features:\n",
    "    summaries: get_summary(user_query, df)\n",
    "    imputation: impute_missing(user_query, df)\n",
    "    outliers: detect_outliers(user_query, df)\n",
    "\n",
    "    Generate Python code that calls the needed functions in order.\n",
    "    Each function takes (user_query, df) and returns modified df.\n",
    "    Store final result in 'df'.\n",
    "    \n",
    "    Example: df = impute_missing(user_query, df)\n",
    "    Or multistep: df = impute_missing(user_query, df); df = get_summary(user_query, df)\n",
    "    \n",
    "    Return only executable Python code, no explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call LLM\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\") # gpt-3.5 for development\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    generated_code = response.content.strip()\n",
    "\n",
    "    # Clean up Code (TODO)\n",
    "\n",
    "    # Execute Code (Split into separate function later maybe)\n",
    "    try:\n",
    "        exec_env = {\n",
    "            'df': df.copy(),\n",
    "            'user_query': user_query,\n",
    "            'get_summary': get_summary,\n",
    "            'impute_missing': impute_missing, # Example\n",
    "            'transform_data': transform_data, # Example\n",
    "            'detect_outliers': detect_outliers, # Example\n",
    "            'pd': pd,\n",
    "            'np': np\n",
    "        }\n",
    "        \n",
    "        exec(generated_code, exec_env)\n",
    "        return exec_env['df']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-cleaning-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
